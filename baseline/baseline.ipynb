{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e0a6f33-eb30-4b39-b0b6-07feb0c797f1",
   "metadata": {},
   "source": [
    "NOTE: Update the `model_paths` list below to match your system.\n",
    "\n",
    "These are absolute paths from the author's machine. \n",
    "If you're running this on your own setup or downloaded models from the GitHub repo, make sure you point to the correct local directories."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6a586a7-e174-40e7-a4ba-8d9623ac7f03",
   "metadata": {},
   "source": [
    "You can use the following:\n",
    "\n",
    "import os\n",
    "\n",
    "base_model_dir = \"saved_models\"\n",
    "model_paths = [\n",
    "    os.path.join(base_model_dir, \"finetuned_0/checkpoint-357\"),\n",
    "    os.path.join(base_model_dir, \"finetuned_1/checkpoint-612\"),\n",
    "    os.path.join(base_model_dir, \"finetuned_2/checkpoint-357\"),\n",
    "    os.path.join(base_model_dir, \"finetuned_3/checkpoint-51\"),\n",
    "    os.path.join(base_model_dir, \"finetuned_4/checkpoint-357\"),\n",
    "    os.path.join(base_model_dir, \"finetuned_5/checkpoint-306\"),\n",
    "    os.path.join(base_model_dir, \"finetuned_6/checkpoint-663\"),\n",
    "    os.path.join(base_model_dir, \"finetuned_7/checkpoint-1020\"),\n",
    "    os.path.join(base_model_dir, \"finetuned_1/checkpoint-663\"),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0bf6a7c-a425-4daa-9056-4d1ffca05a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-01 16:44:42.926696: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-01 16:44:43.547486: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-01 16:44:43.773020: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-04-01 16:44:49.577296: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2025-04-01 16:44:49.577514: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2025-04-01 16:44:49.577523: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset loaded with 1130 samples.\n",
      "\n",
      "===== Loading fine-tuned model from: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_0/checkpoint-357 =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_0/checkpoint-357\n",
      "  Accuracy:  0.8053\n",
      "  F1 Score:  0.8052\n",
      "  Precision: 0.8068\n",
      "  Recall:    0.8056\n",
      "\n",
      "===== Loading fine-tuned model from: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_1/checkpoint-612/ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_1/checkpoint-612/\n",
      "  Accuracy:  0.7788\n",
      "  F1 Score:  0.7787\n",
      "  Precision: 0.7787\n",
      "  Recall:    0.7787\n",
      "\n",
      "===== Loading fine-tuned model from: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_2/checkpoint-357/ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_2/checkpoint-357/\n",
      "  Accuracy:  0.7788\n",
      "  F1 Score:  0.7787\n",
      "  Precision: 0.7795\n",
      "  Recall:    0.7790\n",
      "\n",
      "===== Loading fine-tuned model from: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_3/checkpoint-51/ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_3/checkpoint-51/\n",
      "  Accuracy:  0.6947\n",
      "  F1 Score:  0.6947\n",
      "  Precision: 0.6947\n",
      "  Recall:    0.6947\n",
      "\n",
      "===== Loading fine-tuned model from: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_4/checkpoint-357/ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_4/checkpoint-357/\n",
      "  Accuracy:  0.7876\n",
      "  F1 Score:  0.7876\n",
      "  Precision: 0.7877\n",
      "  Recall:    0.7877\n",
      "\n",
      "===== Loading fine-tuned model from: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_5/checkpoint-306/ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_5/checkpoint-306/\n",
      "  Accuracy:  0.7699\n",
      "  F1 Score:  0.7698\n",
      "  Precision: 0.7706\n",
      "  Recall:    0.7701\n",
      "\n",
      "===== Loading fine-tuned model from: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_6/checkpoint-663/ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_6/checkpoint-663/\n",
      "  Accuracy:  0.7699\n",
      "  F1 Score:  0.7695\n",
      "  Precision: 0.7729\n",
      "  Recall:    0.7704\n",
      "\n",
      "===== Loading fine-tuned model from: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_7/checkpoint-1020/ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_7/checkpoint-1020/\n",
      "  Accuracy:  0.7965\n",
      "  F1 Score:  0.7965\n",
      "  Precision: 0.7965\n",
      "  Recall:    0.7965\n",
      "\n",
      "===== Loading fine-tuned model from: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_8/checkpoint-663/ =====\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: /home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_8/checkpoint-663/\n",
      "  Accuracy:  0.8053\n",
      "  F1 Score:  0.8053\n",
      "  Precision: 0.8056\n",
      "  Recall:    0.8055\n",
      "\n",
      "===== Final Averages Across All Models =====\n",
      "Avg Accuracy:  0.7763\n",
      "Avg F1 Score:  0.7762\n",
      "Avg Precision: 0.7770\n",
      "Avg Recall:    0.7765\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    XLMRobertaTokenizer,\n",
    "    XLMRobertaForSequenceClassification\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score\n",
    ")\n",
    "\n",
    "############################\n",
    "# 1) DEFINE MODEL PATHS\n",
    "############################\n",
    "model_paths = [\n",
    "    \"/home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_0/checkpoint-357\",\n",
    "    \"/home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_1/checkpoint-612/\",\n",
    "    \"/home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_2/checkpoint-357/\",\n",
    "    \"/home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_3/checkpoint-51/\",\n",
    "    \"/home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_4/checkpoint-357/\",\n",
    "    \"/home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_5/checkpoint-306/\",\n",
    "    \"/home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_6/checkpoint-663/\",\n",
    "    \"/home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_7/checkpoint-1020/\",\n",
    "    \"/home/biyikh/Euphemisms/OPET_NOPET_experiment/tr_train_xlmr_NOPETs_splits/saved_models/finetuned_8/checkpoint-663/\"]\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "############################\n",
    "# 2) LOAD DATASET ONCE\n",
    "############################\n",
    "df = pd.read_csv(\"TR_OPETs.csv\")  # or TR_OPETs, etc.\n",
    "df = df.sample(frac=1, random_state=42).reset_index(drop=True)  \n",
    "print(\"Dataset loaded with\", len(df), \"samples.\")\n",
    "\n",
    "texts = df[\"text\"].tolist()\n",
    "labels = df[\"label\"].tolist()\n",
    "\n",
    "############################\n",
    "# 3) PREPARE TO STORE METRICS\n",
    "############################\n",
    "acc_scores = []\n",
    "f1_scores = []\n",
    "prec_scores = []\n",
    "rec_scores = []\n",
    "\n",
    "############################\n",
    "# 4) TOKENIZER (FROM BASE)\n",
    "############################\n",
    "tokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\")\n",
    "\n",
    "############################\n",
    "# 5) LOOP OVER MODEL PATHS\n",
    "############################\n",
    "for model_path in model_paths:\n",
    "    print(f\"\\n===== Loading fine-tuned model from: {model_path} =====\")\n",
    "\n",
    "    # Load the fine-tuned XLM-R model\n",
    "    model = XLMRobertaForSequenceClassification.from_pretrained(model_path)\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "\n",
    "    ########################################\n",
    "    # Generate embeddings for each example\n",
    "    ########################################\n",
    "    feature_list = []\n",
    "    for text in tqdm(texts, desc=\"Extracting embeddings\", leave=False):\n",
    "        inputs = tokenizer(\n",
    "            text,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\",\n",
    "            max_length=512\n",
    "        ).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # We'll extract the base roberta embeddings\n",
    "            # (like your approach: model.roberta(**inputs))\n",
    "            outputs = model.roberta(**inputs)  \n",
    "            last_hidden_state = outputs.last_hidden_state  # [batch, seq_len, hidden_size]\n",
    "            # Mean pooling\n",
    "            embedding = last_hidden_state.mean(dim=1).squeeze()\n",
    "        \n",
    "        feature_list.append(embedding.cpu().numpy())\n",
    "\n",
    "    X = np.array(feature_list, dtype=np.float32)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Train-test split\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "\n",
    "    # Logistic Regression training\n",
    "    lr_clf = LogisticRegression(random_state=42, solver='liblinear')\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "\n",
    "    # Predictions\n",
    "    y_pred = lr_clf.predict(X_test)\n",
    "\n",
    "    # Evaluate\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    f1  = f1_score(y_test, y_pred, average='macro')\n",
    "    prec = precision_score(y_test, y_pred, average='macro')\n",
    "    rec  = recall_score(y_test, y_pred, average='macro')\n",
    "\n",
    "    print(f\"Model: {model_path}\")\n",
    "    print(f\"  Accuracy:  {acc:.4f}\")\n",
    "    print(f\"  F1 Score:  {f1:.4f}\")\n",
    "    print(f\"  Precision: {prec:.4f}\")\n",
    "    print(f\"  Recall:    {rec:.4f}\")\n",
    "\n",
    "    # Store results\n",
    "    acc_scores.append(acc)\n",
    "    f1_scores.append(f1)\n",
    "    prec_scores.append(prec)\n",
    "    rec_scores.append(rec)\n",
    "\n",
    "############################\n",
    "# 6) PRINT AVERAGE METRICS\n",
    "############################\n",
    "print(\"\\n===== Final Averages Across All Models =====\")\n",
    "avg_acc  = np.mean(acc_scores)\n",
    "avg_f1   = np.mean(f1_scores)\n",
    "avg_prec = np.mean(prec_scores)\n",
    "avg_rec  = np.mean(rec_scores)\n",
    "\n",
    "print(f\"Avg Accuracy:  {avg_acc:.4f}\")\n",
    "print(f\"Avg F1 Score:  {avg_f1:.4f}\")\n",
    "print(f\"Avg Precision: {avg_prec:.4f}\")\n",
    "print(f\"Avg Recall:    {avg_rec:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
